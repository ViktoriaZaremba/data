{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Reading Excel file\n",
    "# bread = pd.read_excel('raw_bread.xlsx')\n",
    "# bread = pd.read_csv('raw_bread.csv')\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "## Here we have transaction data, which include column, Date,Time,Transaction,Item\n",
    "## we should remove duplicate transaction, it shows quantity of item in same transaction,\n",
    "## it is not needed in appriori aglo as we only care about different item in particular transaction\n",
    "# bread"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "## dropping Duplicate Transaction\n",
    "# bread = bread.drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "## we need to split transaction data into Dataframe/tabular structure as follow\n",
    "# new = bread['Date,Time,Transaction,Item'].str.split(',', n = 3, expand = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "## assigning column to data frame \"bread\"\n",
    "# bread['Date'] = new[0]\n",
    "# bread['Time'] = new[1]\n",
    "# bread['Transaction'] = new[2]\n",
    "# bread['Item'] = new[3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# in this dataframe we only need column Trasaction and Item, rest is not needed in association mining rule\n",
    "# bread[['Date', 'Time', 'Transaction', 'Item']].head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# we need to convert cloumn transacton & item into Crosstab or we can say Binary Matrix as follow\n",
    "# transaction = pd.crosstab(index= bread['Transaction'], columns= bread['Item'])\n",
    "# transaction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "## Just writing cdv file to check result\n",
    "## I came to know that, we have one unwanted column named \"NONE\", we should remove it as follow and proceed further\n",
    "#tab.to_csv('tab.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "## removing unwanted col \"NONE\"\n",
    "# transaction = transaction.drop(['NONE'], axis = 1)\n",
    "transaction = pd.read_csv('raw_bread_c.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def APRIORI_MY(data, min_support=0.04,  max_length = 4):\n",
    "    # Collecting Required Library\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from itertools import combinations\n",
    "    # Step 1:\n",
    "    # Creating a dictionary to stored support of an itemset.\n",
    "    support = {}\n",
    "    L = list(data.columns)\n",
    "\n",
    "    # Step 2:\n",
    "    #generating combination of items with len i in ith iteration\n",
    "    for i in range(1, max_length+1):\n",
    "        c = set(combinations(L,i))\n",
    "\n",
    "    # Reset \"L\" for next ith iteration\n",
    "        L =set()\n",
    "    # Step 3:\n",
    "        #iterate through each item in \"c\"\n",
    "        for j in list(c):\n",
    "            #print(j)\n",
    "            sup = data.loc[:,j].product(axis=1).sum()/len(data.index)\n",
    "            if sup > min_support:\n",
    "                #print(sup, j)\n",
    "                support[j] = sup\n",
    "\n",
    "                # Appending frequent itemset in list \"L\", already reset list \"L\"\n",
    "                L = list(set(L) | set(j))\n",
    "\n",
    "    # Step 4: data frame with cols \"items\", 'support'\n",
    "    result = pd.DataFrame(list(support.items()), columns = [\"Items\", \"Support\"])\n",
    "    return(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Creating APRIORI_MY function to generate frequent itesets based on minimum threshold support = 0.02\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    Items      Support\n110095                  (Juice, Vegan Feast, Transaction)  4842.489875\n105214  (Adjustment, Afternoon with the baker, Transac...  4842.489875\n6035               (Pick and Mix Bowls, Tea, Transaction)  4842.489875\n60080         (Alfajores, Lemon and coconut, Transaction)  4842.489875\n89320                   (Duck egg, Mortimer, Transaction)  4842.489875\n...                                                   ...          ...\n49953                                 (Tshirt, NONE, Jam)     1.000000\n49952      (Bare Popcorn, Mighty Protein, Olum & polenta)     1.000000\n49951             (Juice, Bowl Nic Pitt, Coffee granules)     1.000000\n49950                (Polenta, Lemon and coconut, Basket)     1.000000\n147535              (Vegan Feast, Valentine's card, Cake)     1.000000\n\n[147536 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Items</th>\n      <th>Support</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>110095</th>\n      <td>(Juice, Vegan Feast, Transaction)</td>\n      <td>4842.489875</td>\n    </tr>\n    <tr>\n      <th>105214</th>\n      <td>(Adjustment, Afternoon with the baker, Transac...</td>\n      <td>4842.489875</td>\n    </tr>\n    <tr>\n      <th>6035</th>\n      <td>(Pick and Mix Bowls, Tea, Transaction)</td>\n      <td>4842.489875</td>\n    </tr>\n    <tr>\n      <th>60080</th>\n      <td>(Alfajores, Lemon and coconut, Transaction)</td>\n      <td>4842.489875</td>\n    </tr>\n    <tr>\n      <th>89320</th>\n      <td>(Duck egg, Mortimer, Transaction)</td>\n      <td>4842.489875</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49953</th>\n      <td>(Tshirt, NONE, Jam)</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>49952</th>\n      <td>(Bare Popcorn, Mighty Protein, Olum &amp; polenta)</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>49951</th>\n      <td>(Juice, Bowl Nic Pitt, Coffee granules)</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>49950</th>\n      <td>(Polenta, Lemon and coconut, Basket)</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>147535</th>\n      <td>(Vegan Feast, Valentine's card, Cake)</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>147536 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## finding frequent itemset with min support = 4%\n",
    "my_freq_itemset = APRIORI_MY(transaction, 0.04, 3)\n",
    "my_freq_itemset.sort_values(by = 'Support', ascending = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Creating ASSOCIATION_RULE_MY function to generate itemset based on minimun threshold confidence.\n",
    "\n",
    "def ASSOCIATION_RULE_MY(df, min_threshold=0.5):\n",
    "    import pandas as pd\n",
    "    from itertools import permutations\n",
    "\n",
    "    # STEP 1:\n",
    "    #creating required varaible\n",
    "    support = pd.Series(df.Support.values, index=df.Items).to_dict()\n",
    "    data = []\n",
    "    L= df.Items.values\n",
    "\n",
    "    # Step 2:\n",
    "    #generating rule using permutation\n",
    "    p = list(permutations(L, 2))\n",
    "\n",
    "    # Iterating through each rule\n",
    "    for i in p:\n",
    "\n",
    "        # If LHS(Antecedent) of rule is subset of RHS then valid rule.\n",
    "        if set(i[0]).issubset(i[1]):\n",
    "            conf = support[i[1]]/support[i[0]]\n",
    "            #print(i, conf)\n",
    "            if conf > min_threshold:\n",
    "                #print(i, conf)\n",
    "                j = i[1][not i[1].index(i[0][0])]\n",
    "                lift = support[i[1]]/(support[i[0]]* support[(j,)])\n",
    "                leverage = support[i[1]] - (support[i[0]]* support[(j,)])\n",
    "                convection = (1 - support[(j,)])/(1- conf)\n",
    "                data.append([i[0], (j,), support[i[0]], support[(j,)], support[i[1]], conf, lift, leverage, convection])\n",
    "\n",
    "\n",
    "    # STEP 3:\n",
    "    result = pd.DataFrame(data, columns = [\"antecedents\", \"consequents\", \"antecedent support\", \"consequent support\",\n",
    "                                        \"support\", \"confidence\", \"Lift\", \"Leverage\", \"Convection\"])\n",
    "    return(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Rule with minimun confidence = 50%\n",
    "my_rule = ASSOCIATION_RULE_MY(my_freq_itemset, 0.5)\n",
    "my_rule"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_rule.sort_values(by='Lift', ascending= False).head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loading standard package\n",
    "from mlxtend.frequent_patterns import apriori, association_rules"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## finding frequent itemset with min support = 4%\n",
    "frequent_itemset = apriori(df = transaction, min_support= 0.04, use_colnames= True)\n",
    "frequent_itemset.sort_values(by = 'support', ascending = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Createing associate rule such that item brought with conditional probability(Confidence) more than 50% with corresponding item"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Rule with minimun confidence = 50%\n",
    "Rules = association_rules(frequent_itemset, min_threshold= 0.5)\n",
    "Rules"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Finally sorting results by Lift to get highly associated itemsets.\n",
    "Rules.sort_values(by='lift', ascending= False).head(10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}